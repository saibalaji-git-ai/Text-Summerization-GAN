This report1 summarizes the content of the NIPS 2016 tutorial on generative
adversarial networks (GANs) (Goodfellow et al., 2014b). The tutorial was designed
primarily to ensure that it answered most of the questions asked by
audience members ahead of time, in order to make sure that the tutorial would
be as useful as possible to the audience. This tutorial is not intended to be
a comprehensive review of the eld of GANs; many excellent papers are not
described here, simply because they were not relevant to answering the most
frequent questions, and because the tutorial was delivered as a two hour oral
presentation and did not have unlimited time cover all subjects.
The tutorial describes: (1) Why generative modeling is a topic worth studying,
(2) how generative models work, and how GANs compare to other generative
models, (3) the details of how GANs work, (4) research frontiers in GANs,
and (5) state-of-the-art image models that combine GANs with other methods.
Finally, the tutorial contains three exercises for readers to complete, and the
solutions to these exercises.
Generative adversarial networks are an example of generative models. The
term \generative model" is used in many dierent ways. In this tutorial, the
term refers to any model that takes a training set, consisting of samples drawn
from a distribution pdata, and learns to represent an estimate of that distribution
somehow. The result is a probability distribution pmodel. In some cases, the
model estimates pmodel explicitly, as shown in gure 1. In other cases, the
model is only able to generate samples from pmodel, as shown in gure 2. Some
models are able to do both. GANs focus primarily on sample generation, though
it is possible to design GANs that can do both.
One might legitimately wonder why generative models are worth studying, especially
generative models that are only capable of generating data rather than
providing an estimate of the density function. After all, when applied to images,
such models seem to merely provide more images, and the world has no shortage
of images.
There are several reasons to study generative models, including:
 Training and sampling from generative models is an excellent test of our
ability to represent and manipulate high-dimensional probability distributions.
High-dimensional probability distributions are important objects in
a wide variety of applied math and engineering domains.
Generative models can be incorporated into reinforcement learning in several
ways. Reinforcement learning algorithms can be divided into two categories;
model-based and model-free, with model-based algorithms being
those that contain a generative model. Generative models of time-series
data can be used to simulate possible futures. Such models could be used
for planning and for reinforcement learning in a variety of ways. A generative
model used for planning can learn a conditional distribution over
future states of the world, given the current state of the world and hypothetical
actions an agent might take as input. The agent can query the
model with dierent potential actions and choose actions that the model
predicts are likely to yield a desired state of the world. For a recent example
of such a model, see Finn et al. (2016b), and for a recent example of
the use of such a model for planning, see Finn and Levine (2016). Another
way that generative models might be used for reinforcement learning is
to enable learning in an imaginary environment, where mistaken actions
do not cause real damage to the agent. Generative models can also be
used to guide exploration by keeping track of how often dierent states
have been visited or dierent actions have been attempted previously.
Generative models, and especially GANs, can also be used for inverse reinforcement
learning.Generative models can be trained with missing data and can provide predictions
on inputs that are missing data. One particularly interesting case
of missing data is semi-supervised learning, in which the labels for many
or even most training examples are missing. Modern deep learning algorithms
typically require extremely many labeled examples to be able to
generalize well. Semi-supervised learning is one strategy for reducing the
number of labels. The learning algorithm can improve its generalization by
studying a large number of unlabeled examples which, which are usually
easier to obtain. Generative models, and GANs in particular, are able
to perform semi-supervised learning reasonably well.Generative models, and GANs in particular, enable machine learning to
work with multi-modal outputs. For many tasks, a single input may correspond
to many dierent correct answers, each of which is acceptable.
Some traditional means of training machine learning models, such as minimizing
the mean squared error between a desired output and the model's
predicted output, are not able to train models that can produce multiple
dierent correct answers.Finally, many tasks intrinsically require realitic generation of samples from
some distribution.
Examples of some of these tasks that intrinsically require the generation of
good samples include:
 Single image super-resolution: In this task, the goal is to take a lowresolution
image and synthesize a high-resolution equivalent. Generative
modeling is required because this task requires the model to impute
more information into the image than was originally there in the input.There are many possible high-resolution images corresponding to the lowresolution
image. The model should choose an image that is a sample from
the probability distribution over possible images. Choosing an image that
is the average of all possible images would yield a result that is too blurry
to be pleasing. See gure 4.
 Tasks where the goal is to create art. Two recent projects have both
demonstrated that generative models, and in particular, GANs, can be
used to create interactive programs that assist the user in creating realistic
images that correspond to rough scenes in the user's imagination.Image-to-image translation applications can convert aerial photos into
maps or convert sketches to images. There is a very long tail of creative
applications that are dicult to anticipate but useful once they have
been discovered.How do generative models work? How do
GANs compare to others?
We now have some idea of what generative models can do and why it might
be desirable to build one. Now we can ask: how does a generative model
actually work? And in particular, how does a GAN work, in comparison to
other generative models?Maximum likelihood estimation
To simplify the discussion somewhat, we will focus on generative models that
work via the principle of maximum likelihood. Not every generative model
uses maximum likelihood. Some generative models do not use maximum likelihood
by default, but can be made to do so (GANs fall into this category). By
ignoring those models that do not use maximum likelihood, and by focusing on
the maximum likelihood version of models that do not usually use maximum
likelihood, we can eliminate some of the more distracting dierences between
dierent models.